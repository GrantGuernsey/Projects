{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twentyFourtyEightNonVisual import GamePlayer\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(prev_state, current_state):\n",
    "    # Calculate the difference in scores between previous and current states\n",
    "    score_diff = current_state.sum() - prev_state.sum()\n",
    "\n",
    "    # Encourage positive score changes\n",
    "    if score_diff > 0:\n",
    "        score_reward = score_diff\n",
    "    else:\n",
    "        score_reward = 0  # No reward for score reduction\n",
    "\n",
    "    # Encourage merging tiles\n",
    "    merge_reward = calculate_merge_reward(prev_state, current_state)\n",
    "\n",
    "    # Encourage keeping empty cells\n",
    "    empty_cells_reward = calculate_empty_cells_reward(prev_state, current_state)\n",
    "\n",
    "    # Encourage reaching larger tiles\n",
    "    max_tile = current_state.max()\n",
    "    if max_tile >= 256:\n",
    "        max_tile_reward = max_tile  # Reward for reaching or exceeding 256\n",
    "    else:\n",
    "        max_tile_reward = 0\n",
    "\n",
    "    # Encourage reaching the maximum tile (2048)\n",
    "    if max_tile >= 2048:\n",
    "        max_score_reward = 2048  # Reward for reaching or exceeding 2048\n",
    "    else:\n",
    "        max_score_reward = 0\n",
    "\n",
    "    # Combine the rewards with different weights\n",
    "    total_reward = (\n",
    "        10 * score_reward\n",
    "        + 1* merge_reward\n",
    "        + 1* empty_cells_reward\n",
    "        + 5 * max_tile_reward\n",
    "        + 2 * max_score_reward\n",
    "    )\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def calculate_merge_reward(prev_state, current_state):\n",
    "    merge_reward = 0\n",
    "    for row in range(4):\n",
    "        for col in range(4):\n",
    "            prev_tile = prev_state[row, col]\n",
    "            current_tile = current_state[row, col]\n",
    "            if current_tile > prev_tile:\n",
    "                merge_reward += current_tile  # Reward for tile merge\n",
    "    return merge_reward\n",
    "\n",
    "def calculate_empty_cells_reward(prev_state, current_state):\n",
    "    empty_cells_reward = 0\n",
    "    empty_cells_prev = np.count_nonzero(prev_state == 0)\n",
    "    empty_cells_current = np.count_nonzero(current_state == 0)\n",
    "    if empty_cells_current > empty_cells_prev:\n",
    "        empty_cells_reward = 1  # Reward for preserving or creating empty cells\n",
    "    return empty_cells_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def train_model(gamer_model, action_space, custom_reward, max_iterations = 500):\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    game_player = GamePlayer()\n",
    "    k = 0\n",
    "    while not game_player.is_game_over() and k < max_iterations:\n",
    "        state = np.array(game_player.get_board())  # Adjust state shape for Keras model\n",
    "        modeled_state = state.reshape(1, 4, 4, 1)  # Reshape to match the expected input shape\n",
    "        \n",
    "        q_values = gamer_model.predict(np.array([state]), verbose=False)\n",
    "        \n",
    "        # Choose the action with the highest Q-value (argmax)\n",
    "        action = np.argmax(q_values)\n",
    "        chosen_action = action_space[action]\n",
    "        print(chosen_action)\n",
    "        all_actions.append(chosen_action)\n",
    "        prev_state = state\n",
    "        state = game_player.move(chosen_action)\n",
    "        reward = custom_reward(prev_state,state, state.max())\n",
    "        all_rewards.append(reward)\n",
    "        all_states.append(prev_state)\n",
    "        k += 1\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    all_states = np.array(all_states)\n",
    "    all_actions = np.array(all_actions)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    gamer_model.fit(all_states, all_actions, sample_weight = all_rewards, batch_size = 1, epochs = 10, verbose = 0)\n",
    "    return all_states[-1].sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "gamer_model = Sequential()\n",
    "# Layer 1: Convolutional layer with 32 filters, a kernel size of (2, 2), and 'valid' padding\n",
    "gamer_model.add(Conv2D(4, kernel_size=(2, 2), padding='valid', activation='relu', input_shape=(4, 4, 1)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters, a kernel size of (2, 2), and 'valid' padding\n",
    "gamer_model.add(Conv2D(16, kernel_size=(2, 2), padding='valid', activation='relu'))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters, a kernel size of (2, 2), and 'valid' padding\n",
    "gamer_model.add(Conv2D(4, kernel_size=(2, 2), padding='valid', activation='relu'))\n",
    "# Layer 4: Flatten the output from the convolutional layers\n",
    "gamer_model.add(Flatten())\n",
    "gamer_model.add(Dense(len(action_space), activation='softmax'))  # Linear activation for Q-values\n",
    "gamer_model.compile(optimizer='adam', loss=loss_fn)  # Use 'mse' for Q-learning\n",
    "\n",
    "gamer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_custom_reward(prev_state, current_state, max_tile):\n",
    "    # Calculate the reward based on the difference in the maximum tile and the number of empty cells\n",
    "    empty_cells = sum(1 for row in current_state for cell in row if cell == 0)\n",
    "    reward_empty_cells = empty_cells * 0.1  # Reward empty cells\n",
    "    \n",
    "    # Check if a merge occurred and reward it\n",
    "    reward_merges = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if prev_state[i][j] == current_state[i][j]:\n",
    "                continue\n",
    "            if current_state[i][j] > prev_state[i][j]:\n",
    "                reward_merges += current_state[i][j] - prev_state[i][j]\n",
    "    \n",
    "    # Reward for achieving the maximum tile (winning the game)\n",
    "    reward_max_tile = 1000 if max_tile >= 2048 else 0\n",
    "    \n",
    "    # Total reward\n",
    "    total_reward = reward_empty_cells + reward_merges + reward_max_tile\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_48 (Conv2D)          (None, 4, 4, 32)          160       \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 4, 4, 64)          8256      \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 4, 4, 128)         32896     \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 312100 (1.19 MB)\n",
      "Trainable params: 312100 (1.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, (2, 2), padding='same', activation='relu', input_shape=(4, 4, 1)))\n",
    "model.add(Conv2D(64, (2, 2), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (2, 2), padding='same', activation='relu'))\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers for action selection\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))  # Use 'linear' activation for Q-values\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "s\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'all_guesses' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Project Folder\\Random\\gamerModel.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episode_number):  \u001b[39m# Train for a certain number of episodes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     action_space \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     score \u001b[39m=\u001b[39m train_model(model, action_space, new_custom_reward, \u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     score_array\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(score_array) \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;32mc:\\Project Folder\\Random\\gamerModel.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Convert lists to NumPy arrays\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m all_states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_states)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m all_guesses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_guesses)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m all_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_rewards)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project%20Folder/Random/gamerModel.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m gamer_model\u001b[39m.\u001b[39mfit(all_states, all_actions, sample_weight \u001b[39m=\u001b[39m all_rewards, batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'all_guesses' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "score_array = []\n",
    "episode_number = 200\n",
    "for _ in range(episode_number):  # Train for a certain number of episodes\n",
    "    action_space = ['w','a','s','d']\n",
    "    score = train_model(model, action_space, new_custom_reward, 100)\n",
    "    score_array.append(score)\n",
    "    if(len(score_array) % 50 == 0):\n",
    "        iterations = list(range(1, len(score_array) + 1))  # Create a list of episode numbers\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(iterations, score_array, marker='o', s=80, c='b', label='Scores')\n",
    "        plt.plot(iterations, score_array, linestyle='--', c='b')\n",
    "\n",
    "        # Calculate the line of best fit (linear regression)\n",
    "        coefficients = np.polyfit(iterations, score_array, 1)\n",
    "        line_of_best_fit = np.poly1d(coefficients)\n",
    "\n",
    "        # Plot the line of best fit\n",
    "        plt.plot(iterations, line_of_best_fit(iterations), c='r', label='Line of Best Fit')\n",
    "\n",
    "        plt.title('Scores vs. Iterations')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Scores')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
